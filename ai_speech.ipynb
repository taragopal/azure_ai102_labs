{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e0ebe5c",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a535794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azure.cognitiveservices.speech as speech_sdk\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "AI_SERVICE_ENDPOINT = os.getenv(\"AI_SERVICE_ENDPOINT\")\n",
    "AI_SERVICE_KEY = os.getenv(\"AI_SERVICE_KEY\")\n",
    "AI_SERVICE_REGION = os.getenv(\"AI_SERVICE_REGION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e01edeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use speech service in: eastus\n"
     ]
    }
   ],
   "source": [
    "speech_config = speech_sdk.SpeechConfig(\n",
    "    subscription=AI_SERVICE_KEY, \n",
    "    region=AI_SERVICE_REGION\n",
    ")\n",
    "print('Ready to use speech service in:', speech_config.region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7187c7cc",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abb0d5c",
   "metadata": {},
   "source": [
    "## Audio transcription (speech-to-text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f084b007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What time is it?\n"
     ]
    }
   ],
   "source": [
    "audio_config = speech_sdk.audio.AudioConfig(filename=\"data/time.wav\")\n",
    "speech_recognizer = speech_sdk.SpeechRecognizer(speech_config, audio_config)\n",
    "\n",
    "speech = speech_recognizer.recognize_once_async().get()\n",
    "\n",
    "if speech.reason == speech_sdk.ResultReason.RecognizedSpeech:\n",
    "    command = speech.text\n",
    "    print(command)\n",
    "else:\n",
    "    print(speech.reason)\n",
    "    if speech.reason == speech_sdk.ResultReason.Canceled:\n",
    "        cancellation = speech.cancellation_details\n",
    "        print(cancellation.reason)\n",
    "        print(cancellation.error_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae98141",
   "metadata": {},
   "source": [
    "## Speech synthesis (text-to-speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44c55976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spoken output saved in data/output.wav\n"
     ]
    }
   ],
   "source": [
    "output_file = \"data/output.wav\"\n",
    "speech_config.speech_synthesis_voice_name = \"en-GB-RyanNeural\"\n",
    "audio_config = speech_sdk.audio.AudioConfig(filename=output_file)\n",
    "speech_synthesizer = speech_sdk.SpeechSynthesizer(speech_config, audio_config)\n",
    "\n",
    "speak = speech_synthesizer.speak_text_async(\"Hello, this audio is generated using the Azure AI Speech.\").get()\n",
    "if speak.reason != speech_sdk.ResultReason.SynthesizingAudioCompleted:\n",
    "    print(speak.reason)\n",
    "else:\n",
    "    print(\"Spoken output saved in \" + output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347fb799",
   "metadata": {},
   "source": [
    "## Speech synthesis with Speech Synthesis Markup Language (SSML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5144267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spoken output saved in data/output_ssml.wav\n"
     ]
    }
   ],
   "source": [
    "ssml = \"\"\"<speak version='1.0' xmlns='http://www.w3.org/2001/10/synthesis' xml:lang='en-US'>\n",
    "  <voice name='en-GB-LibbyNeural'> \n",
    "    <prosody rate=\"medium\" pitch=\"+1st\">\n",
    "      Hello, this audio is generated using the Azure AI Speech.\n",
    "    </prosody>\n",
    "  </voice>\n",
    "</speak>\"\"\"\n",
    "\n",
    "output_file = \"data/output_ssml.wav\"\n",
    "ssml_audio_config = speech_sdk.audio.AudioConfig(filename=output_file)\n",
    "ssml_speech_synthesizer = speech_sdk.SpeechSynthesizer(speech_config, ssml_audio_config)\n",
    "\n",
    "speak = ssml_speech_synthesizer.speak_ssml_async(ssml).get()\n",
    "if speak.reason != speech_sdk.ResultReason.SynthesizingAudioCompleted:\n",
    "    print(speak.reason)\n",
    "else:\n",
    "    print(\"Spoken output saved in \" + output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c058f60",
   "metadata": {},
   "source": [
    "## Speech translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c20ae678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to translate from en-US\n"
     ]
    }
   ],
   "source": [
    "translation_config = speech_sdk.translation.SpeechTranslationConfig(\n",
    "    subscription=AI_SERVICE_KEY,\n",
    "    region=AI_SERVICE_REGION\n",
    ")\n",
    "translation_config.speech_recognition_language = 'en-US'\n",
    "translation_config.add_target_language('fr')\n",
    "translation_config.add_target_language('vi')\n",
    "translation_config.add_target_language('ja')\n",
    "print('Ready to translate from',translation_config.speech_recognition_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5af85962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating \"What time is it?\"\n",
      "Result:\n",
      "\tfr: Quelle heure est-il?\n",
      "\tvi: Mấy giờ rồi?\n",
      "\tja: 何時ですか。\n",
      "\n",
      "TranslationRecognitionResult(result_id=aa0074bf70564e9e9c0ebaea5a06e59c, translations={'fr': 'Quelle heure est-il?', 'vi': 'Mấy giờ rồi?', 'ja': '何時ですか。'}, reason=ResultReason.TranslatedSpeech)\n"
     ]
    }
   ],
   "source": [
    "audio_config_in = speech_sdk.AudioConfig(filename=\"data/time.wav\")\n",
    "translator = speech_sdk.translation.TranslationRecognizer(translation_config=translation_config,\n",
    "                                                          audio_config=audio_config_in)\n",
    "result = translator.recognize_once_async().get()\n",
    "print('Translating \"{}\"'.format(result.text))\n",
    "\n",
    "translations = {}\n",
    "\n",
    "print(\"Result:\")\n",
    "for language, text in result.translations.items():\n",
    "    print(f\"\\t{language}: {text}\")\n",
    "    translations[language] = text\n",
    "print()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262e1b98",
   "metadata": {},
   "source": [
    "## Translation synthesis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95234ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n",
      "Spoken output saved in data/output_fr.wav\n",
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n",
      "Spoken output saved in data/output_vi.wav\n",
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n",
      "Spoken output saved in data/output_ja.wav\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: received close frame, sending a close response frame.\n",
      "Info: on_underlying_io_close_sent: uws_client=0x1078126e0, io_send_result:0\n",
      "Info: on_underlying_io_close_sent: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n",
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: received close frame, sending a close response frame.\n",
      "Info: on_underlying_io_close_sent: uws_client=0x150a4a020, io_send_result:0\n",
      "Info: on_underlying_io_close_sent: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n"
     ]
    }
   ],
   "source": [
    "output_file = \"data/output_{}.wav\"\n",
    "voices = {\n",
    "        \"fr\": \"fr-FR-HenriNeural\",\n",
    "        \"vi\": \"en-GB-AdaMultilingualNeural\",\n",
    "        \"ja\": \"en-GB-AdaMultilingualNeural\"\n",
    "}\n",
    "\n",
    "for language, voice in voices.items():\n",
    "    speech_config.speech_synthesis_voice_name = voice\n",
    "    audio_config_out = speech_sdk.audio.AudioConfig(filename=output_file.format(language))\n",
    "    speech_synthesizer = speech_sdk.SpeechSynthesizer(speech_config, audio_config_out)\n",
    "    speak = speech_synthesizer.speak_text_async(translations.get(language)).get()\n",
    "    if speak.reason != speech_sdk.ResultReason.SynthesizingAudioCompleted:\n",
    "        print(speak.reason)\n",
    "    else:\n",
    "        print(\"Spoken output saved in \" + output_file.format(language))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766281fa",
   "metadata": {},
   "source": [
    "## Diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db89d7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def conversation_transcriber_recognition_canceled_cb(evt: speech_sdk.SessionEventArgs):\n",
    "    print('Canceled event')\n",
    "\n",
    "def conversation_transcriber_session_stopped_cb(evt: speech_sdk.SessionEventArgs):\n",
    "    print('SessionStopped event')\n",
    "\n",
    "def conversation_transcriber_transcribed_cb(evt: speech_sdk.SpeechRecognitionEventArgs):\n",
    "    print('\\nTRANSCRIBED:')\n",
    "    if evt.result.reason == speech_sdk.ResultReason.RecognizedSpeech:\n",
    "        print('\\tText={}'.format(evt.result.text))\n",
    "        print('\\tSpeaker ID={}\\n'.format(evt.result.speaker_id))\n",
    "    elif evt.result.reason == speech_sdk.ResultReason.NoMatch:\n",
    "        print('\\tNOMATCH: Speech could not be TRANSCRIBED: {}'.format(evt.result.no_match_details))\n",
    "\n",
    "def conversation_transcriber_transcribing_cb(evt: speech_sdk.SpeechRecognitionEventArgs):\n",
    "    print('TRANSCRIBING:')\n",
    "    print('\\tText={}'.format(evt.result.text))\n",
    "    print('\\tSpeaker ID={}'.format(evt.result.speaker_id))\n",
    "\n",
    "def conversation_transcriber_session_started_cb(evt: speech_sdk.SessionEventArgs):\n",
    "    print('SessionStarted event')\n",
    "\n",
    "def recognize_from_file():\n",
    "    speech_config = speech_sdk.SpeechConfig(subscription=AI_SERVICE_KEY, region=AI_SERVICE_REGION)\n",
    "    speech_config.speech_recognition_language=\"en-US\"\n",
    "    speech_config.set_property(property_id=speech_sdk.PropertyId.SpeechServiceResponse_DiarizeIntermediateResults, value='true')\n",
    "\n",
    "    audio_config = speech_sdk.audio.AudioConfig(filename=\"data/katiesteve.wav\")\n",
    "    conversation_transcriber = speech_sdk.transcription.ConversationTranscriber(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    transcribing_stop = False\n",
    "\n",
    "    def stop_cb(evt: speech_sdk.SessionEventArgs):\n",
    "        \"\"\"callback that signals to stop continuous recognition upon receiving an event `evt`\"\"\"\n",
    "        print('CLOSING on {}'.format(evt))\n",
    "        nonlocal transcribing_stop\n",
    "        transcribing_stop = True\n",
    "\n",
    "    # Connect callbacks to the events fired by the conversation transcriber\n",
    "    conversation_transcriber.transcribed.connect(conversation_transcriber_transcribed_cb)\n",
    "    # conversation_transcriber.transcribing.connect(conversation_transcriber_transcribing_cb)\n",
    "    conversation_transcriber.session_started.connect(conversation_transcriber_session_started_cb)\n",
    "    conversation_transcriber.session_stopped.connect(conversation_transcriber_session_stopped_cb)\n",
    "    conversation_transcriber.canceled.connect(conversation_transcriber_recognition_canceled_cb)\n",
    "    # stop transcribing on either session stopped or canceled events\n",
    "    conversation_transcriber.session_stopped.connect(stop_cb)\n",
    "    conversation_transcriber.canceled.connect(stop_cb)\n",
    "\n",
    "    conversation_transcriber.start_transcribing_async()\n",
    "\n",
    "    # Waits for completion.\n",
    "    while not transcribing_stop:\n",
    "        time.sleep(.5)\n",
    "\n",
    "    conversation_transcriber.stop_transcribing_async()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd5787c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SessionStarted event\n",
      "\n",
      "TRANSCRIBED:\n",
      "\tText=Good morning, Steve. Good morning, Katie. Have you tried the latest real time diarization in Microsoft Speech Service, which can tell you who said what in real time?\n",
      "\tSpeaker ID=Guest-1\n",
      "\n",
      "\n",
      "TRANSCRIBED:\n",
      "\tText=Not yet. I've been using the batch transcription with diarization functionality, but it produces diarization result until whole audio get processed. Is the new feature can diarise in real time?\n",
      "\tSpeaker ID=Guest-1\n",
      "\n",
      "\n",
      "TRANSCRIBED:\n",
      "\tText=Absolutely.\n",
      "\tSpeaker ID=Guest-1\n",
      "\n",
      "\n",
      "TRANSCRIBED:\n",
      "\tText=That's exciting. Let me try it right now.\n",
      "\tSpeaker ID=Guest-2\n",
      "\n",
      "\n",
      "TRANSCRIBED:\n",
      "\tText=\n",
      "\tSpeaker ID=Unknown\n",
      "\n",
      "\n",
      "TRANSCRIBED:\n",
      "\tText=\n",
      "\tSpeaker ID=Unknown\n",
      "\n",
      "Canceled event\n",
      "CLOSING on ConversationTranscriptionCanceledEventArgs(session_id=b76ebe02a7614eeaa3c93594fe488daa, result=ConversationTranscriptionResult(result_id=037f685746a84cdf91ad336e424e120e, speaker_id=, text=, reason=ResultReason.Canceled))\n",
      "SessionStopped event\n",
      "CLOSING on SessionEventArgs(session_id=b76ebe02a7614eeaa3c93594fe488daa)\n",
      "Info: on_underlying_io_bytes_received: Close frame received\n",
      "Info: on_underlying_io_bytes_received: closing underlying io.\n",
      "Info: on_underlying_io_close_complete: uws_state: 6.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    recognize_from_file()\n",
    "except Exception as err:\n",
    "    print(\"Encountered exception. {}\".format(err))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
